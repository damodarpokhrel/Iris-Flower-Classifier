{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccd87659-b6d0-4291-889f-8dfdbd7f6626",
   "metadata": {},
   "source": [
    "### Beginner to Intermediate Data Science Projects in Python\n",
    "\n",
    "Below are six beginner to intermediate data science projects in Python, each designed to solidify key concepts across the requested topics: Exploratory Data Analysis (EDA) and Hypothesis Testing, Regression Analysis, Sentiment Analysis, Classification, Clustering, and Real-time ML Model Deployment. Each project includes a detailed question, step-by-step instructions, and links to resources or solutions where available. These projects are accessible, practical, and help build a strong foundation in data science.\n",
    "\n",
    "#### Key Points\n",
    "- Research suggests that hands-on projects are effective for learning data science concepts like EDA, regression, and machine learning.\n",
    "- It seems likely that projects using real-world datasets and Python libraries (e.g., Pandas, Scikit-learn) provide practical experience for beginners and intermediates.\n",
    "- The evidence leans toward selecting projects that progressively build skills, starting with data analysis and advancing to model deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244501c2-cbb1-4323-9b78-5e478dda345f",
   "metadata": {},
   "source": [
    "#### Project 1: A/B Testing for Website Conversion Optimization\n",
    "**Question:** Can you conduct an A/B test to determine if a new website theme (dark vs. light) improves user engagement metrics like Click-Through Rate (CTR) and Conversion Rate compared to the old theme?\n",
    "\n",
    "- **Description:** This project involves analyzing user interaction data from two website themes to assess which performs better. You’ll use EDA to explore data distributions and hypothesis testing to compare metrics statistically.\n",
    "- **Why It’s Useful:** Teaches data cleaning, visualization, and statistical testing, foundational for data-driven decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d67cea-a742-41c1-a8bd-1438f1315dff",
   "metadata": {},
   "source": [
    "#### Project 2: Stock Price Prediction System\n",
    "**Question:** Can you build a system to predict future stock prices using historical data and linear regression?\n",
    "\n",
    "- **Description:** This project uses historical stock price data to train a linear regression model for forecasting future prices, introducing regression analysis and time-series data handling.\n",
    "- **Why It’s Useful:** Covers data preprocessing, feature engineering, and regression modeling, key for predictive analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd30608-c605-40ea-a41a-bf5c15cfd118",
   "metadata": {},
   "source": [
    "#### Project 3: Amazon Product Reviews Sentiment Analysis\n",
    "**Question:** Can you analyze Amazon product reviews to classify them as positive or negative using sentiment analysis techniques?\n",
    "\n",
    "- **Description:** This project builds a model to classify customer reviews as positive or negative using natural language processing (NLP) techniques like TF-IDF and Support Vector Machines (SVM).\n",
    "- **Why It’s Useful:** Introduces NLP, text preprocessing, and classification, essential for understanding customer feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a757cb82-e1b3-4a93-9d9b-7fedc875ac3c",
   "metadata": {},
   "source": [
    "#### Project 4: Loan Eligibility Prediction\n",
    "**Question:** Can you develop a model to predict whether a loan applicant will repay their loan based on credit history and other features?\n",
    "\n",
    "- **Description:** This project involves building a binary classification model to predict loan repayment likelihood using features like credit score and income.\n",
    "- **Why It’s Useful:** Teaches classification algorithms, feature selection, and model evaluation, critical for financial applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a254b87-98de-43e0-a730-40b9f413e127",
   "metadata": {},
   "source": [
    "#### Project 5: Customer Segmentation\n",
    "**Question:** Can you segment customers based on demographics and behavior to identify target groups for marketing?\n",
    "\n",
    "- **Description:** This project uses clustering to group customers by attributes like age, income, and spending score, enabling targeted marketing strategies.\n",
    "- **Why It’s Useful:** Covers unsupervised learning and clustering, useful for market analysis and personalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef7a27d-41ce-4724-b0df-9cb1d613de68",
   "metadata": {},
   "source": [
    "#### Project 6: Iris Flower Classification Web App\n",
    "**Question:** Can you build a web app that predicts Iris flower types based on user-inputted measurements using a deployed machine learning model?\n",
    "\n",
    "- **Description:** This project trains a classification model on the Iris dataset and deploys it as a web app using Streamlit for real-time predictions.\n",
    "- **Why It’s Useful:** Introduces model deployment and web app development, bridging data science with production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addc3b46-6f93-449a-8839-4bb105eb53ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Comprehensive Analysis of Data Science Projects\n",
    "\n",
    "This section provides a detailed exploration of six beginner to intermediate data science projects in Python, each tailored to solidify key concepts across Exploratory Data Analysis (EDA) and Hypothesis Testing, Regression Analysis, Sentiment Analysis, Classification, Clustering, and Real-time ML Model Deployment. These projects are designed to be accessible, practical, and progressively challenging, ensuring a comprehensive learning experience. Each project includes a detailed question, step-by-step instructions, and links to resources or solutions, supported by research from reputable educational platforms.\n",
    "\n",
    "#### Background and Selection Process\n",
    "The goal is to provide projects that cover essential data science concepts while being suitable for beginners to intermediates. To achieve this, a thorough review of resources from platforms like Analytics Vidhya, The Clever Programmer, ProjectPro, Upgrad, and 365 Data Science was conducted. These sources were selected for their educational focus and provision of practical project ideas with datasets and code. The projects were chosen to balance simplicity for beginners with challenges for intermediates, ensuring they use Python and real-world datasets to maximize learning outcomes.\n",
    "\n",
    "#### Detailed Project Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b85837-1835-4f20-9383-75da44e019c6",
   "metadata": {},
   "source": [
    "##### 1. Exploratory Data Analysis (EDA) and Hypothesis Testing\n",
    "**Project Question:** Can you conduct an A/B test to determine if a new website theme (dark vs. light) improves user engagement metrics like Click-Through Rate (CTR) and Conversion Rate compared to the old theme?\n",
    "\n",
    "- **Description:**\n",
    "  This project analyzes user interaction data from two website themes (dark and light) to determine which performs better in terms of engagement metrics such as CTR, Conversion Rate, Bounce Rate, and Session Duration. It involves EDA to explore data distributions and hypothesis testing to statistically compare the themes.\n",
    "\n",
    "- **Instructions:**\n",
    "  1. **Data Collection:** Obtain a dataset with user interaction metrics for both themes, such as the one available at [Light Theme and Dark Theme Case Study](https://statso.io/light-theme-and-dark-theme-case-study/). The dataset includes 1000 rows with columns like Theme, CTR, Conversion Rate, Bounce Rate, Scroll Depth, Age, Location, Session_Duration, Purchases, and Added_to_Cart.\n",
    "  2. **Exploratory Data Analysis (EDA):**\n",
    "     - Check for missing values using Pandas (`df.isnull().sum()`) and handle them (e.g., imputation or removal).\n",
    "     - Compute descriptive statistics (`df.describe()`) for numerical columns.\n",
    "     - Visualize data using Matplotlib/Seaborn: create scatter plots (CTR vs. Conversion Rate), histograms (CTR distribution), and box plots (Bounce Rate).\n",
    "  3. **Hypothesis Testing:**\n",
    "     - For Conversion Rate (Purchases), perform a two-sample proportion test using `statsmodels.stats.proportion.proportions_ztest`.\n",
    "     - For Session Duration, perform a two-sample t-test using `scipy.stats.ttest_ind`.\n",
    "     - Set the significance level (α) to 0.05 and interpret p-values to determine if differences are statistically significant.\n",
    "  4. **Conclusion:** Summarize findings and recommend whether to adopt the new theme based on test results.\n",
    "  5. **Tools:** Python, Pandas, NumPy, Matplotlib, Seaborn, SciPy, Statsmodels.\n",
    "\n",
    "- **Educational Value:**\n",
    "  This project teaches data cleaning, visualization, and statistical testing, foundational for data-driven decision-making. It introduces hypothesis testing concepts like null and alternative hypotheses, p-values, and significance levels, which are critical for validating assumptions.\n",
    "\n",
    "- **Link to Solution:**\n",
    "  [A/B Testing of Themes Using Python](https://thecleverprogrammer.com/2023/07/24/a-b-testing-of-themes-using-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebbac887-e6ad-49a8-8559-501375e760e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme                 0\n",
      "Click Through Rate    0\n",
      "Conversion Rate       0\n",
      "Bounce Rate           0\n",
      "Scroll_Depth          0\n",
      "Age                   0\n",
      "Location              0\n",
      "Session_Duration      0\n",
      "Purchases             0\n",
      "Added_to_Cart         0\n",
      "dtype: int64\n",
      "       Click Through Rate  Conversion Rate  Bounce Rate  Scroll_Depth  \\\n",
      "count         1000.000000      1000.000000  1000.000000   1000.000000   \n",
      "mean             0.256048         0.253312     0.505758     50.319494   \n",
      "std              0.139265         0.139092     0.172195     16.895269   \n",
      "min              0.010767         0.010881     0.200720     20.011738   \n",
      "25%              0.140794         0.131564     0.353609     35.655167   \n",
      "50%              0.253715         0.252823     0.514049     51.130712   \n",
      "75%              0.370674         0.373040     0.648557     64.666258   \n",
      "max              0.499989         0.498916     0.799658     79.997108   \n",
      "\n",
      "               Age  Session_Duration  \n",
      "count  1000.000000       1000.000000  \n",
      "mean     41.528000        924.999000  \n",
      "std      14.114334        508.231723  \n",
      "min      18.000000         38.000000  \n",
      "25%      29.000000        466.500000  \n",
      "50%      42.000000        931.000000  \n",
      "75%      54.000000       1375.250000  \n",
      "max      65.000000       1797.000000  \n",
      "Conversion Rate Z-test: stat=nan, p-value=nan\n",
      "Session Duration T-test: stat=nan, p-value=nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/statsmodels/stats/proportion.py:1004: RuntimeWarning: invalid value encountered in divide\n",
      "  prop = count * 1. / nobs\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/statsmodels/stats/proportion.py:1018: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  p_pooled = np.sum(count) * 1. / np.sum(nobs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/statsmodels/stats/proportion.py:1020: RuntimeWarning: divide by zero encountered in divide\n",
      "  nobs_fact = np.sum(1. / nobs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/scipy/_lib/deprecation.py:234: SmallSampleWarning: One or more sample arguments is too small; all returned values will be NaN. See documentation for sample size requirements.\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('website_theme_data.csv')  # Replace with actual dataset path\n",
    "\n",
    "# EDA: Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Descriptive statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Visualizations\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Click Through Rate', y='Conversion Rate', hue='Theme', data=df)\n",
    "plt.title('CTR vs Conversion Rate by Theme')\n",
    "plt.savefig('ctr_vs_conversion.png')\n",
    "plt.close()\n",
    "\n",
    "# Hypothesis Testing: Conversion Rate (Purchases)\n",
    "light_purchases = df[df['Theme'] == 'Light']['Purchases'].sum()\n",
    "dark_purchases = df[df['Theme'] == 'Dark']['Purchases'].sum()\n",
    "light_n = df[df['Theme'] == 'Light'].shape[0]\n",
    "dark_n = df[df['Theme'] == 'Dark'].shape[0]\n",
    "stat, pval = proportions_ztest([light_purchases, dark_purchases], [light_n, dark_n])\n",
    "print(f'Conversion Rate Z-test: stat={stat}, p-value={pval}')\n",
    "\n",
    "# Hypothesis Testing: Session Duration\n",
    "light_duration = df[df['Theme'] == 'Light']['Session_Duration']\n",
    "dark_duration = df[df['Theme'] == 'Dark']['Session_Duration']\n",
    "t_stat, t_pval = ttest_ind(light_duration, dark_duration)\n",
    "print(f'Session Duration T-test: stat={t_stat}, p-value={t_pval}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a435cccf-8e10-438f-83b2-9348bb00cbb8",
   "metadata": {},
   "source": [
    "##### 2. Regression Analysis\n",
    "**Project Question:** Can you build a system to predict future stock prices using historical data and linear regression?\n",
    "\n",
    "- **Description:**\n",
    "  This project uses historical stock price data to train a linear regression model for forecasting future stock prices. It involves data preprocessing, feature engineering, and model evaluation, introducing regression analysis and time-series data handling.\n",
    "\n",
    "- **Instructions:**\n",
    "  1. **Data Collection:** Use the `yfinance` library to download historical stock price data (e.g., for a company like Apple: `AAPL`) from Yahoo Finance.\n",
    "  2. **Data Preprocessing:**\n",
    "     - Handle missing values using interpolation or removal.\n",
    "     - Normalize numerical features if necessary (e.g., using `StandardScaler`).\n",
    "  3. **Feature Engineering:**\n",
    "     - Create features like trading volume, 7-day moving average, and daily price range.\n",
    "  4. **Model Building:**\n",
    "     - Split data into training and testing sets (e.g., 80/20).\n",
    "     - Train a linear regression model using Scikit-learn (`LinearRegression`).\n",
    "     - Evaluate using metrics like Mean Squared Error (MSE) and R-squared.\n",
    "  5. **Prediction:** Use the model to predict future stock prices for a specified period.\n",
    "  6. **Visualization:** Plot actual vs. predicted prices using Matplotlib.\n",
    "  7. **Tools:** Python, Pandas, NumPy, Scikit-learn, yfinance, Matplotlib.\n",
    "\n",
    "- **Educational Value:**\n",
    "  This project covers data preprocessing, feature engineering, and regression modeling, key for predictive analytics. It also introduces time-series data, which is common in financial applications.\n",
    "\n",
    "- **Link to Solution:**\n",
    "  [Linear Regression Project Ideas](https://www.upgrad.com/blog/linear-regression-project-ideas-topics-for-beginners/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f59fd07-69aa-462a-9ba9-33914e921829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Downloading yfinance-0.2.58-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.31 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from yfinance) (2.32.3)\n",
      "Collecting multitasking>=0.0.7 (from yfinance)\n",
      "  Downloading multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from yfinance) (4.3.6)\n",
      "Requirement already satisfied: pytz>=2022.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from yfinance) (2025.1)\n",
      "Collecting frozendict>=2.3.4 (from yfinance)\n",
      "  Downloading frozendict-2.4.6-py313-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from yfinance) (3.17.9)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from yfinance) (4.12.3)\n",
      "Collecting curl_cffi>=0.7 (from yfinance)\n",
      "  Downloading curl_cffi-0.10.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
      "Requirement already satisfied: cffi>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from curl_cffi>=0.7->yfinance) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.31->yfinance) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests>=2.31->yfinance) (2.3.0)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
      "Downloading yfinance-0.2.58-py2.py3-none-any.whl (113 kB)\n",
      "Downloading curl_cffi-0.10.0-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading frozendict-2.4.6-py313-none-any.whl (16 kB)\n",
      "Downloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Installing collected packages: multitasking, frozendict, curl_cffi, yfinance\n",
      "Successfully installed curl_cffi-0.10.0 frozendict-2.4.6 multitasking-0.0.11 yfinance-0.2.58\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9a6e521-2aa3-42b7-84e3-54010d23246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 17.310420490222604, R2: 0.8435106740908065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/wd/hjtm8j014ssf90kvk6rjl1pw0000gn/T/ipykernel_18695/73418955.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Moving_Avg_7'] = df['Close'].rolling(window=7).mean()\n",
      "/var/folders/wd/hjtm8j014ssf90kvk6rjl1pw0000gn/T/ipykernel_18695/73418955.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Price_Range'] = stock['High'] - stock['Low']\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download stock data\n",
    "stock = yf.download('AAPL', start='2020-01-01', end='2023-01-01')\n",
    "df = stock[['Close', 'Volume']]\n",
    "\n",
    "# Feature Engineering\n",
    "df['Moving_Avg_7'] = df['Close'].rolling(window=7).mean()\n",
    "df['Price_Range'] = stock['High'] - stock['Low']\n",
    "df = df.dropna()\n",
    "\n",
    "# Prepare data\n",
    "X = df[['Volume', 'Moving_Avg_7', 'Price_Range']]\n",
    "y = df['Close']\n",
    "train_size = int(len(df) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'MSE: {mse}, R2: {r2}')\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test.index, y_test, label='Actual')\n",
    "plt.plot(y_test.index, y_pred, label='Predicted')\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.legend()\n",
    "plt.savefig('stock_prediction.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fbf3e7-13a7-4fe2-ad35-6a08b4dd08cd",
   "metadata": {},
   "source": [
    "##### 3. Sentiment Analysis\n",
    "**Project Question:** Can you analyze Amazon product reviews to classify them as positive or negative using sentiment analysis techniques?\n",
    "\n",
    "- **Description:**\n",
    "  This project builds a sentiment analysis model to classify Amazon product reviews as positive or negative using NLP techniques like TF-IDF vectorization and a classification model (e.g., SVM). It involves text preprocessing and model evaluation.\n",
    "\n",
    "- **Instructions:**\n",
    "  1. **Data Collection:** Use the Amazon Product Reviews Dataset from [Sentiment Labelled Sentences](http://www.cs.jhu.edu/~mdredze/datasets/sentiment/).\n",
    "  2. **Text Preprocessing:**\n",
    "     - Remove punctuation, stop words, and convert text to lowercase using NLTK or spaCy.\n",
    "     - Perform tokenization and lemmatization.\n",
    "  3. **Feature Extraction:**\n",
    "     - Apply TF-IDF vectorization using `TfidfVectorizer` from Scikit-learn.\n",
    "  4. **Model Training:**\n",
    "     - Train an SVM classifier (`SVC`) or Logistic Regression.\n",
    "     - Split data into training and testing sets (80/20).\n",
    "  5. **Evaluation:**\n",
    "     - Use accuracy, precision, recall, and F1-score to evaluate performance.\n",
    "  6. **Visualization:** Create a confusion matrix to visualize classification results.\n",
    "  7. **Tools:** Python, Pandas, NLTK, spaCy, Scikit-learn, Matplotlib, Seaborn.\n",
    "\n",
    "- **Educational Value:**\n",
    "  This project introduces NLP, text preprocessing, and classification, essential for analyzing customer feedback and social media data.\n",
    "\n",
    "- **Link to Solution:**\n",
    "  [E-commerce Product Reviews Sentiment Analysis](https://www.projectpro.io/project-use-case/ecommerce-product-reviews-ranking-sentiment-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f886290-4b58-494b-9297-0db35d1783e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('amazon_reviews.csv')  # Replace with actual dataset path\n",
    "X = df['review']\n",
    "y = df['sentiment']  # Assuming binary labels (positive/negative)\n",
    "\n",
    "# Preprocess text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "X = X.apply(preprocess)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Visualize confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8a755c-1c4b-4fd6-9df1-bd5345d03352",
   "metadata": {},
   "source": [
    "##### 4. Classification-based Projects\n",
    "**Project Question:** Can you develop a model to predict whether a loan applicant will repay their loan based on credit history and other features?\n",
    "\n",
    "- **Description:**\n",
    "  This project builds a binary classification model to predict loan repayment likelihood using features like credit score, income, and loan amount. It involves data preprocessing, feature selection, and model evaluation.\n",
    "\n",
    "- **Instructions:**\n",
    "  1. **Data Collection:** Use the Loan Prediction Dataset from Kaggle ([Loan Data Set](https://www.kaggle.com/burak3ergun/loan-data-set)).\n",
    "  2. **Data Preprocessing:**\n",
    "     - Handle missing values (e.g., impute with mean/median or remove).\n",
    "     - Encode categorical variables using one-hot encoding (`pd.get_dummies`).\n",
    "  3. **Feature Selection:** Use correlation analysis or feature importance to select relevant features.\n",
    "  4. **Model Training:**\n",
    "     - Train a Support Vector Classifier (SVC) or Random Forest using Scikit-learn.\n",
    "     - Use cross-validation to ensure robustness.\n",
    "  5. **Evaluation:**\n",
    "     - Evaluate using accuracy, ROC curve, and Area Under Curve (AUC).\n",
    "  6. **Visualization:** Plot the ROC curve using Matplotlib.\n",
    "  7. **Tools:** Python, Pandas, NumPy, Scikit-learn, Matplotlib.\n",
    "\n",
    "- **Educational Value:**\n",
    "  This project teaches classification algorithms, feature selection, and model evaluation, critical for financial and risk assessment applications.\n",
    "\n",
    "- **Link to Solution:**\n",
    "  [Loan Prediction Analytics](https://www.projectpro.io/project-use-case/loan-prediction-analytics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5cdb45-f1ef-4dc9-affc-d7edb561f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('loan_data.csv')  # Replace with actual dataset path\n",
    "\n",
    "# Preprocess data\n",
    "df = df.dropna()  # Simple handling of missing values\n",
    "df = pd.get_dummies(df, drop_first=True)  # Encode categorical variables\n",
    "\n",
    "# Features and target\n",
    "X = df.drop('Loan_Status', axis=1)  # Assuming 'Loan_Status' is the target\n",
    "y = df['Loan_Status']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVC\n",
    "model = SVC(probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(f'Cross-validation scores: {scores.mean()}')\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "# ROC Curve\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.savefig('roc_curve.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca7fe76-efcf-41e6-8188-2cecbf6baeae",
   "metadata": {},
   "source": [
    "##### 5. Clustering based Project\n",
    "**Project Question:** Can you segment customers based on demographics (age, gender) and behavior (annual income, spending score) to identify target groups for marketing?\n",
    "\n",
    "- **Description:**\n",
    "  This project uses K-Means clustering to group customers based on attributes like age, gender, annual income, and spending score, enabling targeted marketing strategies. It involves data preprocessing, clustering, and visualization.\n",
    "\n",
    "- **Instructions:**\n",
    "  1. **Data Collection:** Use the Customer Segmentation Dataset from Kaggle ([Customer Segmentation Tutorial](https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python)).\n",
    "  2. **Data Preprocessing:**\n",
    "     - Normalize numerical features using `StandardScaler`.\n",
    "     - Encode categorical variables if necessary.\n",
    "  3. **Clustering:**\n",
    "     - Apply K-Means clustering using Scikit-learn (`KMeans`).\n",
    "     - Use the Elbow Method or Silhouette Score to determine the optimal number of clusters.\n",
    "  4. **Visualization:**\n",
    "     - Create scatter plots or 3D plots to visualize clusters using Matplotlib or Seaborn.\n",
    "  5. **Interpretation:** Analyze cluster characteristics to propose marketing strategies.\n",
    "  6. **Tools:** Python, Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn.\n",
    "\n",
    "- **Educational Value:**\n",
    "  This project covers unsupervised learning and clustering, useful for market analysis, customer personalization, and recommendation systems.\n",
    "\n",
    "- **Link to Solution:**\n",
    "  [Clustering Projects in Machine Learning](https://www.projectpro.io/article/clustering-projects-in-machine-learning/636)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46754a9e-f8ba-406e-b98b-16e77ee9e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('Mall_Customers.csv')  # Replace with actual dataset path\n",
    "\n",
    "# Preprocess data\n",
    "X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Determine optimal number of clusters\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.savefig('elbow_method.png')\n",
    "plt.close()\n",
    "\n",
    "# Apply K-Means\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "df['Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Visualize clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)', hue='Cluster', data=df, palette='viridis')\n",
    "plt.title('Customer Segments')\n",
    "plt.savefig('customer_segments.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bfb6ff-9d00-483b-89b9-378f25682783",
   "metadata": {},
   "source": [
    "##### 6. Real-time ML Model Deployment\n",
    "**Project Question:** Can you build a web application that predicts the type of Iris flower (Setosa, Versicolor, or Virginica) based on user-inputted petal and sepal measurements using a deployed machine learning model?\n",
    "\n",
    "- **Description:**\n",
    "  This project trains a classification model on the Iris dataset and deploys it as a web app using Streamlit, allowing real-time predictions based on user inputs. It bridges data science with production environments.\n",
    "\n",
    "- **Instructions:**\n",
    "  1. **Data Collection:** Use the Iris dataset from UCI Machine Learning Repository ([Iris Dataset](https://archive.ics.uci.edu/ml/datasets/Iris)).\n",
    "  2. **Model Training:**\n",
    "     - Train a Logistic Regression or Decision Tree classifier using Scikit-learn.\n",
    "     - Save the model using `joblib`.\n",
    "  3. **Web App Development:**\n",
    "     - Create a Streamlit app with input fields for petal length, petal width, sepal length, and sepal width.\n",
    "     - Load the saved model and make predictions based on user inputs.\n",
    "  4. **Deployment:**\n",
    "     - Deploy the app on Streamlit.io by linking it to a GitHub repository.\n",
    "     - Ensure `requirements.txt` includes dependencies (e.g., `joblib==0.14.1`, `streamlit==1.7.0`, `scikit-learn==0.23.1`, `pandas==1.0.5`).\n",
    "  5. **Testing:** Test the app by inputting sample measurements and verifying predictions.\n",
    "  6. **Tools:** Python, Pandas, Scikit-learn, joblib, Streamlit.\n",
    "\n",
    "- **Educational Value:**\n",
    "  This project introduces model deployment and web app development, teaching how to make machine learning models accessible in real-world applications.\n",
    "\n",
    "- **Link to Solution:**\n",
    "  [Deploying Machine Learning Models with Streamlit](https://365datascience.com/tutorials/machine-learning-tutorials/how-to-deploy-machine-learning-models-with-python-and-streamlit/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca6d0c9-a93b-4f60-a60d-14abd0d4395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load and train model (for demo; in practice, train separately)\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "joblib.dump(model, 'iris_model.joblib')\n",
    "\n",
    "# Streamlit app\n",
    "st.title('Iris Flower Classification')\n",
    "st.write('Enter measurements to predict the Iris flower type.')\n",
    "\n",
    "# Input fields\n",
    "sepal_length = st.slider('Sepal Length (cm)', 4.0, 8.0, 5.0)\n",
    "sepal_width = st.slider('Sepal Width (cm)', 2.0, 5.0, 3.0)\n",
    "petal_length = st.slider('Petal Length (cm)', 1.0, 7.0, 4.0)\n",
    "petal_width = st.slider('Petal Width (cm)', 0.1, 3.0, 1.0)\n",
    "\n",
    "# Predict\n",
    "model = joblib.load('iris_model.joblib')\n",
    "input_data = [[sepal_length, sepal_width, petal_length, petal_width]]\n",
    "prediction = model.predict(input_data)[0]\n",
    "flower_types = iris.target_names\n",
    "result = flower_types[prediction]\n",
    "\n",
    "st.write(f'Predicted Flower Type: **{result.capitalize()}**')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff5503-aef2-4573-9cdd-174b2b3da957",
   "metadata": {},
   "source": [
    "#### Comparative Analysis\n",
    "The following table summarizes the projects, their difficulty, concepts covered, and recommended progression:\n",
    "\n",
    "| Project Name | Difficulty | Concepts Covered | Recommended Progression |\n",
    "|--------------|------------|------------------|-------------------------|\n",
    "| A/B Testing for Website Conversion | Beginner | EDA, Hypothesis Testing, Visualization | Start here for basics |\n",
    "| Stock Price Prediction | Beginner-Intermediate | Regression, Feature Engineering, Time-Series | After EDA |\n",
    "| Amazon Product Reviews Sentiment Analysis | Intermediate | NLP, Text Preprocessing, Classification | After regression |\n",
    "| Loan Eligibility Prediction | Intermediate | Classification, Feature Selection, Model Evaluation | After sentiment analysis |\n",
    "| Customer Segmentation | Intermediate | Clustering, Unsupervised Learning, Visualization | After classification |\n",
    "| Iris Flower Classification Web App | Intermediate | Classification, Model Deployment, Web App Development | After clustering |\n",
    "\n",
    "This progression starts with foundational skills (EDA, hypothesis testing) and advances to more complex tasks (NLP, clustering, deployment), ensuring a comprehensive learning path.\n",
    "\n",
    "#### Additional Considerations\n",
    "- **Prerequisites:** Familiarity with Python and libraries like Pandas, NumPy, Scikit-learn, Matplotlib, and NLTK is recommended. Beginners can start with tutorials on these libraries.\n",
    "- **Datasets:** Most projects use publicly available datasets from Kaggle or UCI, making them accessible.\n",
    "- **Tools:** All projects use Python, ensuring consistency. Streamlit is used for deployment, which is beginner-friendly.\n",
    "- **Portfolio Building:** Completing these projects creates a strong portfolio, showcasing skills in data analysis, machine learning, and deployment.\n",
    "\n",
    "#### Conclusion\n",
    "These six projects provide a structured path to master data science concepts using Python. They cover a wide range of techniques, from EDA and regression to advanced topics like NLP and model deployment, ensuring a holistic learning experience. By following the instructions and leveraging the provided resources, learners can build practical skills and a robust portfolio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
